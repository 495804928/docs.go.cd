### Overview
Go has a very strong modelling capability in terms of parallel-serial-parallel-serial (pipelinestage-job-task) division of work. It supports multiple SCM / pipeline dependencies & allows fetching of artifacts from ancestor pipelines.

Go has good visualisation of things from pipeline level, i.e. everything from pipeline level was well organised for information consumption.
* what revisions of each direct upstream SCM / pipeline dependency did the pipeline run with (materials tab)
* which stage ran after which one (stage details), how much time did the each stage take (stage graph)
* which jobs ran on which agent, how much time did each job take, status of job (jobs tab in stage details)
* what was the setup that each task ran under i.e. OS, environment variables etc. & what was the output of each task (console.log)

But the visualisation at work-flow level was missing, i.e. if user had to know which revision of ancestor upstream pipeline/SCM did the current pipeline run with he had to trace the path manually. It was also not possible to check revisions of all the upstream SCMs at once. To solve this problem we had to come up with a visualisation where the whole work-flow of current pipeline (upstream & downstream) could be layed out as a graph. This visualisation would also help user know where in downstream his commit is.

### Challenges
When we set out to implement the feature we faced quiet a few challenges:

#### Deterministic vs Randomised Algorithm:
The algorithm had to be deterministic i.e. given a particular input, it should always produce the same output. This would keep the layout same when user would refresh VSM page. It would also keep the nodes at same position until there is change in configuration making it intuitive for human eye to search for a node.

#### Efficiency:
The algorithm had to run in polynomial time since it would be run frequently (every VSM page load). It should be fast enough that we could put a auto-refresh if we wanted to at some point of time.

#### Reusable:
The algorithm had to be scalable & reusable so that we could use it to display the whole of config as a Pipeline Dependency Graph (PDG).

#### Visualisation:
The algorithm should facilitate showing each dependency as separate line, i.e. not merge dependencies into one line so that we could show artifact being fetched from different pipeline separately. But it should minimise edge crossings so as to improve readability / visual appeal.

### Graph Representation & Instance Data population
To be able to implement the algorithm we needed to have Graph representation of whole upstream & downstream flow with respect to current pipeline. Building the graph for VSM is more tricky than it first seems like.

When you are building upstream graph you need to consider the build-cause that is stored in database for current pipeline & recursively traverse upstream build-causes to generate the whole upstream graph.

While for building downstream graph we need to see the config to see all pipelines that get triggered off of current pipeline & recurse downstream till we build the whole downstream graph.

Pipeline instances of upstream pipelines & SCM revisions are got during traversal of build cause, the pipeline instances of downstream pipelines are not directly available in any form. So we query for each pipeline instance we know (starting from current pipeline) - instances of downstream pipeline that ran off of current instance. Once we know the pipeline instances we populate stage information for pipeline instances.

### Algorithm
The Pipeline dependency graph is essentially a Directed Acyclic Graph (DAG). This would mean any algorithm for laying out DAGs would work.

We decided to implement Sugiyama Algorithm for this. The algorithm goes about laying out the graph in 4 steps:
1. Layer Assignment
2. Dummy Node Creation
3. Edge Cross Minimisation
4. Optimisation
